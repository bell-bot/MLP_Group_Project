{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compare import match_audio, LossType\n",
    "from Preprocessing.pre_processing import *\n",
    "from Preprocessing.sliding_windows import create_sliding_windows\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "RECORDINGS_METADATA_PATH = './recording_examples/recordings_metadata.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing files to evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the following packages on your machine (feel free to create a local environment): `gitpython`, \n",
    "\n",
    "then run `python3 generate_csv_from_recordings.py`\n",
    "\n",
    "#### Recordings_metadata headers:\n",
    "- `original_recording_id`: Generated a unique recording id, truncuated from file path for conciseness\n",
    "- `example_recording_id`: Generated a unique recording id, truncuated from file path for conciseness\n",
    "- `example_category`: Defines the folder (Named as categories - i.e) samples, exact_recordings, and imperfect_examples that the recorded example belongs to.\n",
    "  - `samples`: Represents recordings where only a phrase is stated from the original one\n",
    "  - `exact_recordings`: Represents recording examples where the full sentence in the original one is re-iterated.\n",
    "  - `imperfect_examples`: Represents hard recording examples sentences that contains key terms from the original recordings, but not exact:\n",
    "    - Original recording transcription: They went through his files and they didn't find anything **v.s** \n",
    "    - Example Recording Transcription: They didn't find the files\n",
    "- `original_transcript`: Transcription of the original recording\n",
    "- `example_transcript`: Transcription of the recorded example\n",
    "- `path_to_original`: Absolute path on the DICE machine to the file\n",
    "- `path_to_example`: Absolute path to each example, assuming root is the top of the github repository\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recording():\n",
    "    def __init__(self, fs, unprocessed_data, recording_id):\n",
    "        self.fs = fs\n",
    "        self.unprocessed_data = unprocessed_data[:,0] #TODO!!!!  <------ Return one dimension only. check if this is ok\n",
    "        self.id = recording_id #Identifier in case we want to refer to csv metadata\n",
    "        \n",
    "\n",
    "\n",
    "class Evaluation():\n",
    "    def __init__(self, path_to_metadata= RECORDINGS_METADATA_PATH):\n",
    "        self.metadata_df = self.read_recordings_metadata(path_to_metadata)\n",
    "        self.metadata_columns = self.metadata_df.columns\n",
    "        self.RATE = 24000 #Used to resample the two audio files to the same sampling rate\n",
    "        self.n_mfcc = None #Used for preprocessing. variable not used at the moment\n",
    "        \n",
    "    def read_recordings_metadata(self, path):\n",
    "        metadata = pd.read_csv(path)\n",
    "        return metadata\n",
    "    \n",
    "        \n",
    "    def read_all_recordings(self):\n",
    "        recordings_truth, recordings_test = [] , [] \n",
    "        for index, row in self.metadata_df.iterrows():\n",
    "            path_original, path_example = row['path_to_original'], row['path_to_example']\n",
    "            id_original , id_example = row['original_recording_id'], row['example_recording_id']\n",
    "            truth_fs, truth_data = read_audio(path_original, \"mp4\")\n",
    "            test_fs, test_data  = read_audio(path_example, \"m4a\")\n",
    "\n",
    "            recordings_truth.append(Recording(truth_fs, truth_data,id_original))\n",
    "            recordings_test.append(Recording(test_fs, test_data, id_example))\n",
    "            \n",
    "\n",
    "        return recordings_truth, recordings_test\n",
    "    \n",
    "    \n",
    "        \n",
    "    def get_transcripts(self,current_example_recording_id):\n",
    "        row = self.metadata_df.loc[self.metadata_df['example_recording_id'] == current_example_recording_id]\n",
    "        display(row)\n",
    "        \n",
    "    ###Start of preprocessing \n",
    "    ##TODO: some stuff could be moved to the preprocessing code.\n",
    "    \n",
    "    ##TODO: Probably move this to preprocessing instead\n",
    "    def trim_silence(self, data_original, data_test):\n",
    "        #Remove silence from beginning and the end\n",
    "        data_original , _ = librosa.effects.trim(data_original, top_db=40)\n",
    "        data_test , _ = librosa.effects.trim(data_test,top_db=40)\n",
    "        return data_original, data_test\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    def preprocess_truth_and_test_data(self, original_unprocessed_data, original_framerates, test_unprocessed_data, test_framerates, target_rate=24000, N_MFCC=None, trim_silence=True):\n",
    "        #TODO: Calls to_mfcc function  from the pre_processing library and preprocesses the audio\n",
    "        if N_MFCC is not None: \n",
    "            pass\n",
    "        #Resample audio file to same sample rate\n",
    "        data_original = resample_audio(original_unprocessed_data, sampling_rate =original_framerates, target_rate = target_rate) \n",
    "        data_test = resample_audio(test_unprocessed_data, sampling_rate =test_framerates, target_rate = target_rate)\n",
    "        if trim_silence:\n",
    "            data_original, data_test= self.trim_silence(data_original, data_test)\n",
    "        print(len(data_original))\n",
    "        print(len(data_test))\n",
    "\n",
    "        #Setting window size according to length of test recording. However, if original recording is shorter, we set it to that instead\n",
    "        if len(data_test) <= len(data_original):\n",
    "            window_size = len(data_test) -1\n",
    "            step_size = int(window_size/2)\n",
    "        else:\n",
    "            window_size = len(data_original) -1\n",
    "            step_size = int(window_size/2)\n",
    "\n",
    "\n",
    "        original_windows = create_sliding_windows(data_original, window_size=window_size, step_size=step_size)\n",
    "        test_windows = create_sliding_windows(data_test, window_size=window_size, step_size=step_size)\n",
    "        return original_windows, test_windows\n",
    "    \n",
    "    \n",
    "    #### Start of evaluation\n",
    "\n",
    "        \n",
    "    def compute_loss(self, original_windows, test_windows, test_recording_id, loss_type):\n",
    "    \n",
    "      \n",
    "        print(\"Original recording sliding window dim: \", original_windows.shape)\n",
    "        print(\"Test recording sliding window dim: \", test_windows.shape)\n",
    "        #display metadata of correspond row\n",
    "        self.get_transcripts(test_recording_id)\n",
    "\n",
    "        #Calculate the MAE or RMSE of the two audio files \n",
    "        res = []\n",
    "        x = match_audio(torch.Tensor(test_windows),torch.Tensor(original_windows),loss_type=loss_type)\n",
    "        res.append(torch.min(x))\n",
    "\n",
    "\n",
    "        res = np.array(res)\n",
    "        print(\"Minimal MAE: \", np.amin(res))\n",
    "        print(\"Window with minimal MAE: \", np.where(res == np.amin(res)))\n",
    "        \n",
    "        ##Function that combines both preprocess function and compute_loss function. \n",
    "    def evaluate_two_audio_data(self, original_unprocessed_data, original_fs, test_unprocessed_data, test_fs, test_recording_id, loss_type = LossType.MAE, target_rate=24000, N_MFCC=None, trim_silence=True):\n",
    "        print(\"*****\")\n",
    "        original_windows, test_windows = preprocess_truth_and_test_data(original_unprocessed_data, original_fs, test_unprocessed_data, test_fs, target_rate, N_MFCC, trim_silence)\n",
    "        self.compute_loss(original_windows, test_windows)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##Utility Functions\n",
    "def reshape_preprocessed_audio(data):\n",
    "    return data.reshape((data.shape[1], data.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Gets all recordings, unprocessed\n",
    "EvalEngine = Evaluation()\n",
    "recordings_truth, recordings_test = EvalEngine.read_all_recordings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST SIMPLE ARRAY\n",
    "TODO! -> Put this as a unit test instead "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  4]\n",
      "  [ 1  5]\n",
      "  [ 2  6]\n",
      "  [ 3  7]]\n",
      "\n",
      " [[ 4  8]\n",
      "  [ 5  9]\n",
      "  [ 6 10]\n",
      "  [ 7 11]]\n",
      "\n",
      " [[ 8 12]\n",
      "  [ 9 13]\n",
      "  [10 14]\n",
      "  [11 15]]]\n",
      "******\n",
      "[[[ 1  5]\n",
      "  [ 2  6]\n",
      "  [ 3  7]\n",
      "  [ 4  8]]\n",
      "\n",
      " [[ 5  9]\n",
      "  [ 6 10]\n",
      "  [ 7 11]\n",
      "  [ 8 12]]\n",
      "\n",
      " [[ 9 13]\n",
      "  [10 14]\n",
      "  [11 15]\n",
      "  [12 16]]]\n"
     ]
    }
   ],
   "source": [
    "### TEST SIMPLE ARRAY\n",
    "simple_truth= [[0,1,2,3],[4,5,6,7],[8,9,10,11],[12,13,14,15]]\n",
    "simple_test = [[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]]\n",
    "truth_windows = create_sliding_windows(simple_truth, window_size=2, step_size=1)\n",
    "test_windows = create_sliding_windows(simple_test, window_size=2, step_size=1)\n",
    "print(truth_windows)\n",
    "print(\"******\")\n",
    "print(test_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9 13]\n",
      " [10 14]\n",
      " [11 15]\n",
      " [12 16]]\n"
     ]
    }
   ],
   "source": [
    "print(test_windows[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.],\n",
       "        [5.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_audio(torch.Tensor(test_windows[2]),torch.Tensor(truth_windows))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
